(Health-LLM) chrisjihee@dgx-a100:~/proj/Health-LLM$ bash finetune.sh
/raid/chrisjihee/miniforge3/envs/Health-LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.28s/it]
type(model)=<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>
/raid/chrisjihee/miniforge3/envs/Health-LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
/raid/chrisjihee/miniforge3/envs/Health-LLM/lib/python3.11/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
  0%|                                                                                                                                                                                                                                                 | 0/20 [00:00<?, ?it/s]/raid/chrisjihee/miniforge3/envs/Health-LLM/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.18545262601121e+23, 'grad_norm': nan, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.05}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.1}
{'train_runtime': 375.1062, 'train_samples_per_second': 8.278, 'train_steps_per_second': 0.053, 'train_loss': 5.92726313005605e+22, 'epoch': 4.1}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [06:15<00:00, 18.76s/it]
output_dir=output/medalpaca-7b-tuned
type(model)=<class 'torch._dynamo.eval_frame.OptimizedModule'>, model.dtype=torch.float16
type(tokenizer)=<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.18it/s]
You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
type(model)=<class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>, model.dtype=torch.float16
type(tokenizer)=<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.46s/it]
Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors
input=The recent 14-days sensor readings show: [Steps]: [2524.0, 2362.0, 2753.0, 3344.0, 2162.0, 1824.0, 2619.0, 2434.0, 2862.0, 1331.0, 1431.0] steps, [Burned Calorories]: [234.0, 222.0, 250.0, 331.0, 242.0, 218.0, 226.0, 226.0, 283.0, 138.0, 127.0] calories, [Resting Heart Rate]: [58.373215675354004, 57.88592052459717, 56.78752517700195, 0.0, 53.55788993835449, 0.0, 53.84395217895508, 53.669076919555664, 0.0, 0.0, 53.70508861541748, 54.71232509613037, 54.91011714935303, 55.746761322021484] beats/min, [SleepMinutes]: [440.0, 498.0, 449.0, 525.0, 387.0, 213.0, 388.0, 425.0, 231.0, 475.0, 368.0, 452.0] minutes, [Mood]: 3 out of 5; What would be the predicted readiness?
instruction=You are a personalized healthcare agent trained to predict readiness which ranges from 0 to 10 based on physiological data and user information.
response=<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>
(Health-LLM) chrisjihee@dgx-a100:~/proj/Health-LLM$
